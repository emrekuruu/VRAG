{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "from datasets import load_dataset\n",
    "import voyageai\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keys/openai_api_key.txt\", \"r\") as file:\n",
    "    openai_key = file.read().strip()\n",
    "\n",
    "with open(\"keys/voyage_api_key.txt\", \"r\") as file:\n",
    "    voyage_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ibm/finqa\", trust_remote_code=True)\n",
    "\n",
    "# Access the splits\n",
    "data = dataset['train'].to_pandas()\n",
    "validation_data = dataset['validation'].to_pandas()\n",
    "test_data = dataset['test'].to_pandas()\n",
    "\n",
    "data = pd.concat([data, validation_data, test_data])\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"id\", \"question\", \"answer\", \"gold_inds\"]]\n",
    "data[\"Company\"] = [row[0] for row in data.id.str.split(\"/\")]\n",
    "data[\"Year\"] = [row[1] for row in data.id.str.split(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/AAP/2006/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29159"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_companies = set(data.Company.unique())\n",
    "\n",
    "needed_years = {}\n",
    "\n",
    "for company in unique_companies:\n",
    "    needed_years[company] = list(data[data.Company == company].Year.unique())\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for company in needed_years.keys():\n",
    "    for year in needed_years[company]:\n",
    "        try:\n",
    "            file_count += len(os.listdir(f\"docs/{company}/{year}/\"))\n",
    "        except:\n",
    "            print(f\"docs/{company}/{year}/\")\n",
    "            \n",
    "file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(data.Company == \"AAL\" ) & (data.Year == \"2014\")]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "if os.path.exists(\"documents.pkl\"):\n",
    "    with open(\"documents.pkl\", \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "        \n",
    "else:\n",
    "    documents = {}\n",
    "    tables = {}\n",
    "\n",
    "    companies = [\"AAL\"]\n",
    "\n",
    "    # Define the function to process a single page\n",
    "    def process_page(company, year, page):\n",
    "        image = convert_from_path(f\"docs/{company}/{year}/{page}\")[0]\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return f\"{company}/{year}/{page}\", text\n",
    "\n",
    "    # Define companies and years\n",
    "    companies = [\"AAL\"]\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        futures = []\n",
    "\n",
    "        for company in companies:\n",
    "            years = os.listdir(f\"docs/{company}/\")\n",
    "            years = [\"2014\"]\n",
    "\n",
    "            for year in years:\n",
    "                pages = os.listdir(f\"docs/{company}/{year}/\")\n",
    "\n",
    "                # Submit tasks for each page\n",
    "                for page in pages:\n",
    "                    futures.append(executor.submit(process_page, company, year, page))\n",
    "\n",
    "        # Gather results\n",
    "        for future in futures:\n",
    "            page_key, text = future.result()\n",
    "            documents[page_key] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = voyageai.Client(api_key=voyage_api_key)\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size  \n",
    "\n",
    "    def embed_document(self, text):\n",
    "        embedding = vo.embed([text], model=\"voyage-3\", input_type=\"document\").embeddings[0]\n",
    "        return embedding\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = vo.embed(batch, model=\"voyage-3\", input_type=\"document\").embeddings\n",
    "            embeddings.extend([embedding for embedding in batch_embeddings])\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        embedding = vo.embed([query], model=\"voyage-3\", input_type=\"query\").embeddings[0]\n",
    "        return embedding\n",
    "    \n",
    "embedder = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \".chroma\"\n",
    "\n",
    "docs = []\n",
    "\n",
    "def process_document(id, text):\n",
    "    local_docs = []\n",
    "    try:\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        company = id.split(\"/\")[0]\n",
    "        year = id.split(\"/\")[1]\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(page_content=chunk, metadata={\"id\": id, \"chunk\": i, \"company\":company, \"year\": year})\n",
    "            local_docs.append(doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {id}: {e}\")\n",
    "    \n",
    "    return local_docs\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_document, id, text): id for id, text in documents.items()}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            docs.extend(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/_dg3pqgn2zdf7f95_1dg07rw0000gn/T/ipykernel_76088/104589934.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(persist_directory=persist_directory, embedding_function=embedder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing ChromaDB from .chroma\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "persist_directory = \".chroma\"\n",
    "\n",
    "if os.path.exists(persist_directory):\n",
    "\n",
    "    # Load the existing ChromaDB\n",
    "    chroma_db = Chroma(persist_directory=persist_directory, embedding_function=embedder)\n",
    "    print(\"Loaded existing ChromaDB from .chroma\")\n",
    "\n",
    "else:\n",
    "\n",
    "    # Create ChromaDB and store the documents\n",
    "    chroma_db = Chroma(\n",
    "        embedding_function=embedder,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    \n",
    "    print(\"Created new ChromaDB and saved to .chroma\")\n",
    "\n",
    "    batch_size = 5000\n",
    "    num_batches = ceil(len(docs) / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(docs))\n",
    "        batch_docs = docs[start_idx:end_idx]\n",
    "        \n",
    "        chroma_db.add_texts(\n",
    "            texts=[doc.page_content for doc in batch_docs],\n",
    "            metadatas=[doc.metadata for doc in batch_docs]\n",
    "        )\n",
    "\n",
    "        print(f\"Batch {i+1} of {num_batches} added to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    Answer the following question based solely on the following context. Give a short answer, 2-3 words at most. Then explain the steps you took to arrive at your answer.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\")\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chroma_db.as_retriever()\n",
    "\n",
    "def format_context(context):\n",
    "    response = \"\"\n",
    "    for doc in context:\n",
    "        response += doc.page_content + \"\\n\\n\"\n",
    "    return response\n",
    "\n",
    "retrieve_chain = retriever | format_context \n",
    "\n",
    "generation_chain = RunnableLambda(lambda input: {\n",
    "    \"context\": retrieve_chain.invoke(input[\"query\"]), \n",
    "    \"query\": input[\"query\"]\n",
    "}) | PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-torch.tensor(x)))\n",
    "\n",
    "def rerank(query, documents, ids, top_k=1):\n",
    "    scores = {}\n",
    "    reranking = vo.rerank(query=query, documents=documents, model=\"rerank-2\", top_k=len(documents))\n",
    "\n",
    "    for i, r in enumerate(reranking.results):\n",
    "        normalized_score = sigmoid(r.relevance_score).item()\n",
    "        scores[ids[i]] = normalized_score\n",
    "\n",
    "    top_scorers = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "    return {id: score for id, score in top_scorers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Retrieved Context\",\"Correct Documents\", \"Generated Answer\", \"Correct Answer\"], index=data.index)\n",
    "\n",
    "# Define a function to process each item\n",
    "def process_item(idx):\n",
    "    query = data.loc[idx, \"question\"]\n",
    "    company = data.loc[idx, \"Company\"]\n",
    "    year = data.loc[idx, \"Year\"]\n",
    "\n",
    "    # Initialize retriever\n",
    "    retriever = chroma_db.as_retriever(search_kwargs={\"k\": 20, \"filter\": {\"$and\": [{\"company\": company}, {\"year\": year}]}})\n",
    "    \n",
    "    # Retrieve and rerank\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    retrieved = rerank(query, [doc.page_content for doc in retrieved_docs], [doc.metadata[\"id\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Populate results\n",
    "    retrieved_context = list(retrieved.keys())[0]\n",
    "    generated_answer = generation_chain.invoke(input={\"query\": query}).content\n",
    "\n",
    "    return idx, retrieved_context, generated_answer\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_item, idx) for idx in data.index]\n",
    "\n",
    "    # Gather results\n",
    "    for future in futures:\n",
    "        idx, retrieved_context, generated_answer = future.result()\n",
    "        results.loc[idx, \"Retrieved Context\"] = retrieved_context\n",
    "        results.loc[idx, \"Generated Answer\"] = generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Correct Answer\"] = data.answer\n",
    "results[\"Correct Documents\"] = data.id\n",
    "results[\"Golden Context\"] = data.gold_inds\n",
    "results.to_csv(\"vanilla.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
