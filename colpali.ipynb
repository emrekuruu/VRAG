{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys/hf_key.txt', 'r') as file:\n",
    "    hf_key = file.read().strip()\n",
    "\n",
    "with open(\"keys/openai_api_key.txt\", \"r\") as file:\n",
    "    openai_key = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ibm/finqa\", trust_remote_code=True)\n",
    "\n",
    "# Access the splits\n",
    "data = dataset['train'].to_pandas()\n",
    "validation_data = dataset['validation'].to_pandas()\n",
    "test_data = dataset['test'].to_pandas()\n",
    "\n",
    "data = pd.concat([data, validation_data, test_data])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data[[\"id\", \"question\", \"answer\", \"gold_inds\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Company\"] = [row[0] for row in data.id.str.split(\"/\")]\n",
    "data[\"Year\"] = [row[1] for row in data.id.str.split(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/AAP/2006/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29159"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_companies = set(data.Company.unique())\n",
    "\n",
    "needed_years = {}\n",
    "\n",
    "for company in unique_companies:\n",
    "    needed_years[company] = list(data[data.Company == company].Year.unique())\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for company in needed_years.keys():\n",
    "    for year in needed_years[company]:\n",
    "        try:\n",
    "            file_count += len(os.listdir(f\"docs/{company}/{year}/\"))\n",
    "        except:\n",
    "            print(f\"docs/{company}/{year}/\")\n",
    "            \n",
    "file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.Company == \"AAL\" )& (data.Year == \"2014\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d540a01703024cb496c2c1c5d845c0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v1.0\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite is on. Deleting existing index finqa to build a new one.\n",
      "Indexing file: docs/temp/page_108.pdf\n",
      "Added page 1 of document 0 to index.\n",
      "Index exported to .byaldi/finqa\n",
      "Index exported to .byaldi/finqa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'docs/temp/page_108.pdf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(\n",
    "    input_path=\"docs/temp/\",\n",
    "    index_name=\"finqa\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define companies\n",
    "companies = [\"AAL\"]\n",
    "\n",
    "# Define a function to process a single page and add it to the RAG index\n",
    "def process_page_for_index(company, year, page):\n",
    "    \n",
    "    RAG.add_to_index(\n",
    "        input_item=f\"docs/{company}/{year}/{page}\",\n",
    "        store_collection_with_index=True,\n",
    "        metadata={\"Company\": company, \"Year\": year, \"Page\": page},\n",
    "    )\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "\n",
    "    for company in companies:\n",
    "        years = os.listdir(f\"docs/{company}/\")\n",
    "        years = [\"2014\"]\n",
    "\n",
    "        for year in years:\n",
    "            pages = os.listdir(f\"docs/{company}/{year}/\")\n",
    "\n",
    "            # Submit tasks for each page\n",
    "            for page in pages:\n",
    "                futures.append(executor.submit(process_page_for_index, company, year, page))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prompt(image, question):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Answer the following query based solely on the provided image,  Give a short answer, 2-3 words at most. Then explain the steps you took to arrive at your answer.\n",
    "\n",
    "    Query: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": query},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/pdf;base64,{image}\"},\n",
    "        },\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Retrieved Context\",\"Correct Documents\", \"Generated Answer\", \"Correct Answer\"], index=data.index)\n",
    "\n",
    "# Define the function to process a single item\n",
    "def process_item(idx):\n",
    "    query = data.loc[idx, \"question\"]\n",
    "    company = data.loc[idx, \"Company\"]\n",
    "    year = data.loc[idx, \"Year\"]\n",
    "\n",
    "    # Perform retrieval\n",
    "    retrieved = RAG.search(query, k=1, filter_metadata={\"Company\": company, \"Year\": year})\n",
    "\n",
    "    # Populate the results row\n",
    "    retrieved_context = f\"{company}/{year}/{retrieved[0].metadata['Page']}\"\n",
    "    generated_answer = model.invoke([image_prompt(retrieved[0][\"base64\"], query)]).content\n",
    "\n",
    "    return idx, retrieved_context, generated_answer\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_item, idx) for idx in data.index]\n",
    "\n",
    "    # Gather results\n",
    "    for future in futures:\n",
    "        idx, retrieved_context, generated_answer = future.result()\n",
    "        results.loc[idx, \"Retrieved Context\"] = retrieved_context\n",
    "        results.loc[idx, \"Generated Answer\"] = generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Correct Answer\"] = data.answer\n",
    "results[\"Correct Documents\"] = data.id\n",
    "results[\"Golden Context\"] = data.gold_inds\n",
    "\n",
    "results.to_csv(\"colpali.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
