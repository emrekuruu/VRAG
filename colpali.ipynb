{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd \n",
    "import asyncio\n",
    "import os\n",
    "import psutil\n",
    "import nest_asyncio\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys/hf_key.txt', 'r') as file:\n",
    "    hf_key = file.read().strip()\n",
    "\n",
    "with open(\"keys/openai_api_key.txt\", \"r\") as file:\n",
    "    openai_key = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ibm/finqa\", trust_remote_code=True)\n",
    "\n",
    "# Access the splits\n",
    "data = dataset['train'].to_pandas()\n",
    "validation_data = dataset['validation'].to_pandas()\n",
    "test_data = dataset['test'].to_pandas()\n",
    "\n",
    "data = pd.concat([data, validation_data, test_data])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data[[\"id\", \"question\", \"answer\", \"gold_inds\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Company\"] = [row[0] for row in data.id.str.split(\"/\")]\n",
    "data[\"Year\"] = [row[1] for row in data.id.str.split(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_companies = set(data.Company.unique())\n",
    "\n",
    "needed_years = {}\n",
    "\n",
    "for company in unique_companies:\n",
    "    needed_years[company] = list(data[data.Company == company].Year.unique())\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for company in needed_years.keys():\n",
    "    for year in needed_years[company]:\n",
    "        try:\n",
    "            file_count += len(os.listdir(f\"docs/{company}/{year}/\"))\n",
    "        except:\n",
    "            print(f\"docs/{company}/{year}/\")\n",
    "            \n",
    "file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.Company == \"AAL\" )& (data.Year == \"2014\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v1.0\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite is on. Deleting existing index finqa to build a new one.\n",
      "Indexing file: docs/temp/page_108.pdf\n",
      "Added page 1 of document 0 to index.\n",
      "Index exported to .byaldi/finqa\n",
      "Index exported to .byaldi/finqa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'docs/temp/page_108.pdf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(\n",
    "    input_path=\"docs/temp/\",\n",
    "    index_name=\"finqa\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 84.4% - Processing AAL/2014/page_93.pdf\n",
      "Memory usage: 84.4% - Processing AAL/2014/page_225.pdf\n",
      "Memory usage: 84.4% - Processing AAL/2014/page_219.pdf\n",
      "Memory usage: 84.4% - Processing AAL/2014/page_218.pdf\n",
      "Added page 1 of document 2 to index.Added page 1 of document 3 to index.\n",
      "Added page 1 of document 1 to index.\n",
      "Added page 1 of document 4 to index.\n",
      "\n",
      "Index exported to .byaldi/finqa\n",
      "Index exported to .byaldi/finqa\n",
      "Memory usage: 92.0% - Processing AAL/2014/page_230.pdf\n",
      "Index exported to .byaldi/finqa\n",
      "Memory usage: 92.0% - Processing AAL/2014/page_224.pdf\n",
      "Memory usage: 92.0% - Processing AAL/2014/page_92.pdf\n",
      "Index exported to .byaldi/finqa\n",
      "Memory usage: 92.0% - Processing AAL/2014/page_193.pdf\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio to enable nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define companies\n",
    "companies = [\"AAL\"]\n",
    "\n",
    "# Define an async function to process a single page\n",
    "async def process_page_for_index(company, year, page):\n",
    "    \n",
    "    print(f\"Memory usage: {psutil.virtual_memory().percent}% - Processing {company}/{year}/{page}\")\n",
    "    \n",
    "    # Simulate asynchronous RAG indexing (replace with actual async-compatible call if possible)\n",
    "    await asyncio.to_thread(\n",
    "        RAG.add_to_index,\n",
    "        input_item=f\"docs/{company}/{year}/{page}\",\n",
    "        store_collection_with_index=True,\n",
    "        metadata={\"Company\": company, \"Year\": year, \"Page\": page},\n",
    "    )\n",
    "\n",
    "# Define an async function to process all companies, years, and pages\n",
    "async def process_all():\n",
    "    tasks = []\n",
    "\n",
    "    for company in companies:\n",
    "        years = os.listdir(f\"docs/{company}/\")\n",
    "        years = [\"2014\"]\n",
    "\n",
    "        for year in years:\n",
    "            pages = os.listdir(f\"docs/{company}/{year}/\")\n",
    "\n",
    "            # Create async tasks for each page\n",
    "            for page in pages:\n",
    "                tasks.append(process_page_for_index(company, year, page))\n",
    "\n",
    "    # Run all tasks concurrently with a limit on concurrency\n",
    "    concurrency_limit = 4\n",
    "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
    "\n",
    "    async def semaphore_task(task):\n",
    "        async with semaphore:\n",
    "            return await task\n",
    "\n",
    "    # Await all tasks\n",
    "    await asyncio.gather(*(semaphore_task(task) for task in tasks))\n",
    "\n",
    "# Run the asyncio event loop within Jupyter\n",
    "await process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prompt(image, question):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Answer the following query based solely on the provided image,  Give a short answer, 2-3 words at most. Then explain the steps you took to arrive at your answer.\n",
    "\n",
    "    Query: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": query},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/pdf;base64,{image}\"},\n",
    "        },\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Retrieved Context\",\"Correct Documents\", \"Generated Answer\", \"Correct Answer\"], index=data.index)\n",
    "\n",
    "# Define the function to process a single item\n",
    "def process_item(idx):\n",
    "    query = data.loc[idx, \"question\"]\n",
    "    company = data.loc[idx, \"Company\"]\n",
    "    year = data.loc[idx, \"Year\"]\n",
    "\n",
    "    # Perform retrieval\n",
    "    retrieved = RAG.search(query, k=1, filter_metadata={\"Company\": company, \"Year\": year})\n",
    "\n",
    "    # Populate the results row\n",
    "    retrieved_context = f\"{company}/{year}/{retrieved[0].metadata['Page']}\"\n",
    "    generated_answer = model.invoke([image_prompt(retrieved[0][\"base64\"], query)]).content\n",
    "\n",
    "    return idx, retrieved_context, generated_answer\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_item, idx) for idx in data.index]\n",
    "\n",
    "    # Gather results\n",
    "    for future in futures:\n",
    "        idx, retrieved_context, generated_answer = future.result()\n",
    "        results.loc[idx, \"Retrieved Context\"] = retrieved_context\n",
    "        results.loc[idx, \"Generated Answer\"] = generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Correct Answer\"] = data.answer\n",
    "results[\"Correct Documents\"] = data.id\n",
    "results[\"Golden Context\"] = data.gold_inds\n",
    "\n",
    "results.to_csv(\"colpali.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
